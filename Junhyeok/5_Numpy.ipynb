{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36938653.70608069\n",
      "1 42022605.86971484\n",
      "2 53347542.47566977\n",
      "3 56787718.61521052\n",
      "4 41685811.33612635\n",
      "5 18915527.652373634\n",
      "6 6514854.601019183\n",
      "7 2676704.623082008\n",
      "8 1631282.8360079944\n",
      "9 1239172.818621689\n",
      "10 1014678.9177368424\n",
      "11 852182.6392907946\n",
      "12 724221.07287433\n",
      "13 620398.3523529854\n",
      "14 535039.3003564056\n",
      "15 464074.840779002\n",
      "16 404576.890765838\n",
      "17 354351.55288099987\n",
      "18 311751.4970871875\n",
      "19 275384.6334858594\n",
      "20 244167.25088529894\n",
      "21 217214.1717086853\n",
      "22 193798.89547011815\n",
      "23 173411.13027632746\n",
      "24 155599.04649761866\n",
      "25 139959.13982216077\n",
      "26 126177.51660642671\n",
      "27 113996.0704086135\n",
      "28 103202.13956531066\n",
      "29 93605.32998137467\n",
      "30 85040.81445282488\n",
      "31 77394.26155759626\n",
      "32 70546.1696249313\n",
      "33 64394.919191902256\n",
      "34 58863.74213576016\n",
      "35 53877.26894287143\n",
      "36 49373.03287397814\n",
      "37 45296.83130964126\n",
      "38 41606.93165748171\n",
      "39 38261.607593800436\n",
      "40 35220.682579914195\n",
      "41 32451.779061963207\n",
      "42 29928.177194909662\n",
      "43 27624.362776133486\n",
      "44 25518.82706310264\n",
      "45 23594.393983983493\n",
      "46 21842.074560932517\n",
      "47 20236.64244178103\n",
      "48 18762.37139050634\n",
      "49 17408.045868213085\n",
      "50 16162.32467399836\n",
      "51 15015.461964729682\n",
      "52 13958.658346169363\n",
      "53 12984.197424656686\n",
      "54 12084.247653726572\n",
      "55 11253.820295223115\n",
      "56 10486.942275544332\n",
      "57 9777.391418065043\n",
      "58 9120.46117172922\n",
      "59 8511.767866411617\n",
      "60 7947.465211335416\n",
      "61 7423.824955670774\n",
      "62 6937.810080921357\n",
      "63 6486.430358685278\n",
      "64 6067.119038772155\n",
      "65 5677.049188846371\n",
      "66 5314.048674250429\n",
      "67 4976.22145875406\n",
      "68 4661.75821437179\n",
      "69 4368.81972991634\n",
      "70 4095.7200599436555\n",
      "71 3841.139929455788\n",
      "72 3603.423429101107\n",
      "73 3381.4968330767433\n",
      "74 3174.3103741518757\n",
      "75 2980.7574080800896\n",
      "76 2799.9071998325153\n",
      "77 2630.786907159644\n",
      "78 2472.5323138243493\n",
      "79 2324.450633304463\n",
      "80 2185.9135188830837\n",
      "81 2056.1448069385733\n",
      "82 1934.5649044505694\n",
      "83 1820.7594167460543\n",
      "84 1713.9612379044956\n",
      "85 1613.8737752242441\n",
      "86 1519.991397745242\n",
      "87 1431.9111621645955\n",
      "88 1349.2526366447676\n",
      "89 1271.6670868290462\n",
      "90 1198.8336821333714\n",
      "91 1130.3831856001007\n",
      "92 1066.054069516838\n",
      "93 1005.5970324422424\n",
      "94 948.7898098019551\n",
      "95 895.387019610384\n",
      "96 845.1229630712564\n",
      "97 797.8454318315416\n",
      "98 753.3504926708406\n",
      "99 711.4731508920704\n",
      "100 672.0574537236744\n",
      "101 634.9202650910338\n",
      "102 599.9414985287069\n",
      "103 566.9949841995362\n",
      "104 535.9469513945489\n",
      "105 506.69129765743446\n",
      "106 479.09850639779665\n",
      "107 453.0841600452711\n",
      "108 428.579415110972\n",
      "109 405.4616225631745\n",
      "110 383.62939230147026\n",
      "111 363.0371274199581\n",
      "112 343.5973817667199\n",
      "113 325.2431170982003\n",
      "114 307.9123219769656\n",
      "115 291.54407187418656\n",
      "116 276.0825706798115\n",
      "117 261.4764635365024\n",
      "118 247.68221802146388\n",
      "119 234.64404006272338\n",
      "120 222.32051859718487\n",
      "121 210.67057201726914\n",
      "122 199.65765981974255\n",
      "123 189.24196471144163\n",
      "124 179.39207284988433\n",
      "125 170.0742617108865\n",
      "126 161.25949044376415\n",
      "127 152.92110689320262\n",
      "128 145.0323123941509\n",
      "129 137.56369822023467\n",
      "130 130.4957969877338\n",
      "131 123.8023956694123\n",
      "132 117.46795040943833\n",
      "133 111.46982251844992\n",
      "134 105.78724168025786\n",
      "135 100.4076240614343\n",
      "136 95.30987164574451\n",
      "137 90.4804279557165\n",
      "138 85.90325660621906\n",
      "139 81.56496974087074\n",
      "140 77.45350890864927\n",
      "141 73.55670725239771\n",
      "142 69.86234458186439\n",
      "143 66.3597887120028\n",
      "144 63.03892526308883\n",
      "145 59.88999491961483\n",
      "146 56.903491904579354\n",
      "147 54.07118429713976\n",
      "148 51.38227786483885\n",
      "149 48.83095488424219\n",
      "150 46.41087088849448\n",
      "151 44.11383181342141\n",
      "152 41.93742972809913\n",
      "153 39.87470606777715\n",
      "154 37.91676824738586\n",
      "155 36.05740519191005\n",
      "156 34.292103278023006\n",
      "157 32.61640699275457\n",
      "158 31.024545921998673\n",
      "159 29.5122870402924\n",
      "160 28.07612704397378\n",
      "161 26.711745618538885\n",
      "162 25.415259241895615\n",
      "163 24.183655850319973\n",
      "164 23.01338938691615\n",
      "165 21.901387060397933\n",
      "166 20.84439798516417\n",
      "167 19.83962171892272\n",
      "168 18.884595766710763\n",
      "169 17.976910428495234\n",
      "170 17.113963069465715\n",
      "171 16.29338287858011\n",
      "172 15.513383896403745\n",
      "173 14.771411511197673\n",
      "174 14.065750356462257\n",
      "175 13.394674847175585\n",
      "176 12.75652123264047\n",
      "177 12.149298521253858\n",
      "178 11.571709552587894\n",
      "179 11.022304038625013\n",
      "180 10.499493466682797\n",
      "181 10.002051570011773\n",
      "182 9.52876271167751\n",
      "183 9.078299331977473\n",
      "184 8.649562643400229\n",
      "185 8.2415603038434\n",
      "186 7.853228028003801\n",
      "187 7.483636873182313\n",
      "188 7.131788692893893\n",
      "189 6.796931698625251\n",
      "190 6.477973970947092\n",
      "191 6.174307010879213\n",
      "192 5.8851793415482625\n",
      "193 5.609870524167488\n",
      "194 5.347678561379333\n",
      "195 5.098030262612115\n",
      "196 4.8603494952455\n",
      "197 4.63397079765066\n",
      "198 4.418238254244074\n",
      "199 4.212709397839173\n",
      "200 4.016909492091379\n",
      "201 3.830403449752086\n",
      "202 3.652700091237775\n",
      "203 3.4834420775175428\n",
      "204 3.322142267753992\n",
      "205 3.168405964638548\n",
      "206 3.0219356880487656\n",
      "207 2.8823496845413747\n",
      "208 2.749366301983855\n",
      "209 2.6226052831818656\n",
      "210 2.5017873178496624\n",
      "211 2.3866157558038985\n",
      "212 2.2768440476950547\n",
      "213 2.1722111232065866\n",
      "214 2.072454038747507\n",
      "215 1.9773969467578314\n",
      "216 1.886783662548288\n",
      "217 1.8003446057866435\n",
      "218 1.7179334027951505\n",
      "219 1.6393410485344024\n",
      "220 1.564398157747677\n",
      "221 1.4929407105234211\n",
      "222 1.424801455945879\n",
      "223 1.3598177602240986\n",
      "224 1.2978347674533501\n",
      "225 1.2387237388373111\n",
      "226 1.1823412632870625\n",
      "227 1.1285643355546786\n",
      "228 1.0772709840182624\n",
      "229 1.0283440761004186\n",
      "230 0.9816667248593368\n",
      "231 0.9371397222187482\n",
      "232 0.8946594114673727\n",
      "233 0.8541291813541154\n",
      "234 0.8154612218878385\n",
      "235 0.7785757772723865\n",
      "236 0.7433703645842351\n",
      "237 0.7097818188652508\n",
      "238 0.6777322732798798\n",
      "239 0.647150090204551\n",
      "240 0.6179604349290736\n",
      "241 0.5901088900764454\n",
      "242 0.5635265616126144\n",
      "243 0.5381592910321042\n",
      "244 0.5139490136192002\n",
      "245 0.49083611558949036\n",
      "246 0.4687773483265746\n",
      "247 0.4477381434543667\n",
      "248 0.4276417847426429\n",
      "249 0.408454347084366\n",
      "250 0.39013961199752284\n",
      "251 0.3726591168182685\n",
      "252 0.35596893845147815\n",
      "253 0.34003289413783644\n",
      "254 0.3248221642003347\n",
      "255 0.3102978974601148\n",
      "256 0.29643038729178983\n",
      "257 0.283188730384653\n",
      "258 0.27054772614055594\n",
      "259 0.2584747207530325\n",
      "260 0.2469477493171494\n",
      "261 0.23594003079353523\n",
      "262 0.22542882294160604\n",
      "263 0.21538955040637298\n",
      "264 0.2058041726681652\n",
      "265 0.1966491974799493\n",
      "266 0.1879050471150168\n",
      "267 0.1795538231746085\n",
      "268 0.1715780118955835\n",
      "269 0.1639604041261768\n",
      "270 0.1566833461886266\n",
      "271 0.1497343161059512\n",
      "272 0.14309558847976211\n",
      "273 0.136753417165856\n",
      "274 0.13069871556377963\n",
      "275 0.12491257793591751\n",
      "276 0.11938469681189673\n",
      "277 0.11410325484826671\n",
      "278 0.10905819283532833\n",
      "279 0.10423849753371703\n",
      "280 0.09963312772444384\n",
      "281 0.09523320748261946\n",
      "282 0.09103022449772927\n",
      "283 0.08701366591014437\n",
      "284 0.0831758833809865\n",
      "285 0.07950945384771564\n",
      "286 0.07600602970705397\n",
      "287 0.07265812403691653\n",
      "288 0.06945912177026095\n",
      "289 0.06640247900803324\n",
      "290 0.06348101601862466\n",
      "291 0.06068935852103186\n",
      "292 0.058021539986269376\n",
      "293 0.055472228066328545\n",
      "294 0.05303579580736917\n",
      "295 0.05070713837025408\n",
      "296 0.04848171744886226\n",
      "297 0.04635477493846477\n",
      "298 0.04432180572622098\n",
      "299 0.04237885470238566\n",
      "300 0.040521866153829016\n",
      "301 0.03874660656943376\n",
      "302 0.037049970624578384\n",
      "303 0.035428323544553886\n",
      "304 0.03387807037789991\n",
      "305 0.03239614884754862\n",
      "306 0.03097971210693974\n",
      "307 0.02962615174685771\n",
      "308 0.028331708926610084\n",
      "309 0.02709419679213485\n",
      "310 0.02591115324114881\n",
      "311 0.02478031568644229\n",
      "312 0.02369899173818847\n",
      "313 0.022665273896843325\n",
      "314 0.021676965964502422\n",
      "315 0.020732161959436217\n",
      "316 0.01982876546821528\n",
      "317 0.01896498000382433\n",
      "318 0.018139229920091662\n",
      "319 0.017349645777390035\n",
      "320 0.016594646441973896\n",
      "321 0.015872714185865007\n",
      "322 0.015182472270934364\n",
      "323 0.01452247741508806\n",
      "324 0.013891274812719439\n",
      "325 0.013287729759877513\n",
      "326 0.012710674729404885\n",
      "327 0.012158794389065284\n",
      "328 0.01163101873707077\n",
      "329 0.011126294873197487\n",
      "330 0.010643714810930198\n",
      "331 0.010182128151851442\n",
      "332 0.009740702872445446\n",
      "333 0.00931852658644972\n",
      "334 0.008914830269610933\n",
      "335 0.008528698708125169\n",
      "336 0.00815939459637449\n",
      "337 0.007806192226706018\n",
      "338 0.007468381776044086\n",
      "339 0.007145273281524999\n",
      "340 0.006836240970532263\n",
      "341 0.006540656682833389\n",
      "342 0.006257955196392297\n",
      "343 0.005987530668913028\n",
      "344 0.0057288538326845\n",
      "345 0.005481432075904331\n",
      "346 0.005244773591696575\n",
      "347 0.005018376708510684\n",
      "348 0.00480181203885024\n",
      "349 0.004594644415345682\n",
      "350 0.004396498407331298\n",
      "351 0.0042069289406741085\n",
      "352 0.0040255885878193775\n",
      "353 0.0038520952755165825\n",
      "354 0.003686144158430194\n",
      "355 0.003527369916759701\n",
      "356 0.0033754760792743966\n",
      "357 0.0032301578768371203\n",
      "358 0.003091138444879529\n",
      "359 0.0029581397906964387\n",
      "360 0.0028308926250286918\n",
      "361 0.002709145293350848\n",
      "362 0.0025926709858785746\n",
      "363 0.0024812283505684825\n",
      "364 0.0023746009938185077\n",
      "365 0.002272576746902712\n",
      "366 0.0021749666125947323\n",
      "367 0.002081574011232208\n",
      "368 0.0019922123393981725\n",
      "369 0.0019067022190654497\n",
      "370 0.0018248839203305924\n",
      "371 0.0017466032347683545\n",
      "372 0.0016716901877053653\n",
      "373 0.0016000088257872168\n",
      "374 0.0015314138220538693\n",
      "375 0.0014657833910050205\n",
      "376 0.0014029760373125554\n",
      "377 0.0013428752020505626\n",
      "378 0.0012853587090707883\n",
      "379 0.0012303230171930527\n",
      "380 0.0011776533827599477\n",
      "381 0.0011272486762848835\n",
      "382 0.001079013003835345\n",
      "383 0.0010328518380989696\n",
      "384 0.0009886784993846944\n",
      "385 0.0009464028326277667\n",
      "386 0.0009059535311956149\n",
      "387 0.000867229765984184\n",
      "388 0.0008301721190385441\n",
      "389 0.0007947032554797269\n",
      "390 0.0007607568237646975\n",
      "391 0.0007282664482912183\n",
      "392 0.000697173806439669\n",
      "393 0.0006674145488503984\n",
      "394 0.0006389302601196607\n",
      "395 0.0006116670508743297\n",
      "396 0.0005855731738095001\n",
      "397 0.0005605992934768009\n",
      "398 0.0005366936336704194\n",
      "399 0.0005138117152551765\n",
      "400 0.0004919103891935339\n",
      "401 0.00047094662238153333\n",
      "402 0.00045088164329764387\n",
      "403 0.00043167520739686755\n",
      "404 0.0004132899771327978\n",
      "405 0.00039569066102375583\n",
      "406 0.00037884534167353977\n",
      "407 0.000362719895218724\n",
      "408 0.00034728347121227047\n",
      "409 0.0003325068782487005\n",
      "410 0.00031836171741877\n",
      "411 0.00030482195353827475\n",
      "412 0.00029186015821406426\n",
      "413 0.00027945175870071467\n",
      "414 0.0002675726811159418\n",
      "415 0.0002562014989967191\n",
      "416 0.00024531545812826294\n",
      "417 0.0002348933569064082\n",
      "418 0.00022491599202933567\n",
      "419 0.00021536453891210272\n",
      "420 0.00020622065574447824\n",
      "421 0.00019746660053249177\n",
      "422 0.00018908540815444814\n",
      "423 0.00018106125055804918\n",
      "424 0.0001733791341849904\n",
      "425 0.00016602467431791533\n",
      "426 0.00015898296702228544\n",
      "427 0.00015224115214845105\n",
      "428 0.00014578623918650065\n",
      "429 0.00013960638078997954\n",
      "430 0.00013369009663113888\n",
      "431 0.00012802493389552983\n",
      "432 0.00012260165579524446\n",
      "433 0.00011740968902037016\n",
      "434 0.00011243710739421522\n",
      "435 0.00010767607158523621\n",
      "436 0.0001031171782787203\n",
      "437 9.87520124575844e-05\n",
      "438 9.457233563118559e-05\n",
      "439 9.057060032882583e-05\n",
      "440 8.67388145193408e-05\n",
      "441 8.306947867565459e-05\n",
      "442 7.955596338206929e-05\n",
      "443 7.619164162958874e-05\n",
      "444 7.297019642928268e-05\n",
      "445 6.98855089094382e-05\n",
      "446 6.693155392549662e-05\n",
      "447 6.410302299970627e-05\n",
      "448 6.139441567194133e-05\n",
      "449 5.8800808119006824e-05\n",
      "450 5.631717559750475e-05\n",
      "451 5.393865585831076e-05\n",
      "452 5.166101225451644e-05\n",
      "453 4.947991203649558e-05\n",
      "454 4.739126223429459e-05\n",
      "455 4.5391092615011986e-05\n",
      "456 4.347559891209976e-05\n",
      "457 4.164121800668647e-05\n",
      "458 3.988462264238905e-05\n",
      "459 3.820228939481564e-05\n",
      "460 3.659123850298027e-05\n",
      "461 3.5048274772585226e-05\n",
      "462 3.357058630822576e-05\n",
      "463 3.215548797122722e-05\n",
      "464 3.0800189379165985e-05\n",
      "465 2.950230958673809e-05\n",
      "466 2.8259209373294786e-05\n",
      "467 2.7068710380602987e-05\n",
      "468 2.5928543748294654e-05\n",
      "469 2.483653673022639e-05\n",
      "470 2.379073071626648e-05\n",
      "471 2.2789060754008596e-05\n",
      "472 2.182971560299926e-05\n",
      "473 2.0910866259535385e-05\n",
      "474 2.0030834101732402e-05\n",
      "475 1.9188000348991066e-05\n",
      "476 1.8380720829575086e-05\n",
      "477 1.7607530047120964e-05\n",
      "478 1.686694886670175e-05\n",
      "479 1.6157601002010645e-05\n",
      "480 1.5478222002451182e-05\n",
      "481 1.482749657059626e-05\n",
      "482 1.4204198103120484e-05\n",
      "483 1.3607174922679238e-05\n",
      "484 1.3035326990677458e-05\n",
      "485 1.2487598822544496e-05\n",
      "486 1.196300469216809e-05\n",
      "487 1.1460497146503337e-05\n",
      "488 1.0979126615520903e-05\n",
      "489 1.0518047770601668e-05\n",
      "490 1.0076394139339221e-05\n",
      "491 9.653366516423084e-06\n",
      "492 9.248142789353168e-06\n",
      "493 8.859968751826263e-06\n",
      "494 8.488138870329618e-06\n",
      "495 8.131958016890211e-06\n",
      "496 7.790786360230236e-06\n",
      "497 7.463989715770465e-06\n",
      "498 7.150917028107241e-06\n",
      "499 6.8510104406708026e-06\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 272.6231994628906\n",
      "199 0.6057168245315552\n",
      "299 0.0032493872568011284\n",
      "399 0.00012183294165879488\n",
      "499 2.8597956770681776e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 299.58367919921875\n",
      "199 0.618004560470581\n",
      "299 0.002940807491540909\n",
      "399 0.00011284475476713851\n",
      "499 2.5068744434975088e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "# requires_grad=False로 설정하여 역전파 중에 이 Tensor들에 대한 변화도를 계산할\n",
    "# 필요가 없음을 나타냅니다. (requres_grad의 기본값이 False이므로 아래 코드에는\n",
    "# 이를 반영하지 않았습니다.)\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "# requires_grad=True로 설정하여 역전파 중에 이 Tensor들에 대한\n",
    "# 변화도를 계산할 필요가 있음을 나타냅니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: Tensor 연산을 사용하여 예상되는 y 값을 계산합니다. 이는 Tensor를\n",
    "    # 사용한 순전파 단계와 완전히 동일하지만, 역전파 단계를 별도로 구현하지 않아도\n",
    "    # 되므로 중간값들에 대한 참조(reference)를 갖고 있을 필요가 없습니다.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Tensor 연산을 사용하여 손실을 계산하고 출력합니다.\n",
    "    # loss는 (1,) 형태의 Tensor이며, loss.item()은 loss의 스칼라 값입니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograd를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를\n",
    "    # 갖는 모든 Tensor에 대해 손실의 변화도를 계산합니다. 이후 w1.grad와 w2.grad는\n",
    "    # w1과 w2 각각에 대한 손실의 변화도를 갖는 Tensor가 됩니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 수동으로 갱신합니다.\n",
    "    # torch.no_grad()로 감싸는 이유는 가중치들이 requires_grad=True이지만\n",
    "    # autograd에서는 이를 추적할 필요가 없기 때문입니다.\n",
    "    # 다른 방법은 weight.data 및 weight.grad.data를 조작하는 방법입니다.\n",
    "    # tensor.data가 tensor의 저장공간을 공유하기는 하지만, 이력을\n",
    "    # 추적하지 않는다는 것을 기억하십시오.\n",
    "    # 또한, 이를 위해 torch.optim.SGD 를 사용할 수도 있습니다.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 871.8598022460938\n",
      "199 7.7912187576293945\n",
      "299 0.11568114906549454\n",
      "399 0.00233393139205873\n",
      "499 0.00016219074313994497\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    torch.autograd.Function을 상속받아 사용자 정의 autograd Function을 구현하고,\n",
    "    Tensor 연산을 하는 순전파와 역전파 단계를 구현하겠습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        순전파 단계에서는 입력을 갖는 Tensor를 받아 출력을 갖는 Tensor를 반환합니다.\n",
    "        ctx는 컨텍스트 객체(context object)로 역전파 연산을 위한 정보 저장에\n",
    "        사용합니다. ctx.save_for_backward method를 사용하여 역전파 단계에서 사용할 어떠한\n",
    "        객체도 저장(cache)해 둘 수 있습니다.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        역전파 단계에서는 출력에 대한 손실의 변화도를 갖는 Tensor를 받고, 입력에\n",
    "        대한 손실의 변화도를 계산합니다.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 사용자 정의 Function을 적용하기 위해 Function.apply 메소드를 사용합니다.\n",
    "    # 여기에 'relu'라는 이름을 붙였습니다.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # 순전파 단계: Tensor 연산을 사용하여 예상되는 y 값을 계산합니다;\n",
    "    # 사용자 정의 autograd 연산을 사용하여 ReLU를 계산합니다.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograde를 사용하여 역전파 단계를 계산합니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 갱신합니다.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1.8761241436004639\n",
      "199 0.02976275235414505\n",
      "299 0.0013562248786911368\n",
      "399 9.374327055411413e-05\n",
      "499 8.072191121755168e-06\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn 패키지를 사용하여 모델을 순차적 계층(sequence of layers)으로 정의합니다.\n",
    "# nn.Sequential은 다른 Module들을 포함하는 Module로, 그 Module들을 순차적으로\n",
    "# 적용하여 출력을 생성합니다. 각각의 Linear Module은 선형 함수를 사용하여\n",
    "# 입력으로부터 출력을 계산하고, 내부 Tensor에 가중치와 편향을 저장합니다.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# 또한 nn 패키지에는 널리 사용하는 손실 함수들에 대한 정의도 포함하고 있습니다;\n",
    "# 여기에서는 평균 제곱 오차(MSE; Mean Squared Error)를 손실 함수로 사용하겠습니다.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다. Module 객체는\n",
    "    # __call__ 연산자를 덮어써(override) 함수처럼 호출할 수 있게 합니다.\n",
    "    # 이렇게 함으로써 입력 데이터의 Tensor를 Module에 전달하여 출력 데이터의\n",
    "    # Tensor를 생성합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다. 예측한 y와 정답인 y를 갖는 Tensor들을 전달하고,\n",
    "    # 손실 함수는 손실 값을 갖는 Tensor를 반환합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계를 실행하기 전에 변화도를 0으로 만듭니다.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를\n",
    "    # 계산합니다. 내부적으로 각 Module의 매개변수는 requires_grad=True 일 때\n",
    "    # Tensor 내에 저장되므로, 이 호출은 모든 모델의 모든 학습 가능한 매개변수의\n",
    "    # 변화도를 계산하게 됩니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다. 각 매개변수는\n",
    "    # Tensor이므로 이전에 했던 것과 같이 변화도에 접근할 수 있습니다.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 61.48438262939453\n",
      "199 1.4286580085754395\n",
      "299 0.016768716275691986\n",
      "399 9.035784751176834e-05\n",
      "499 1.78295238129067e-07\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn 패키지를 사용하여 모델과 손실 함수를 정의합니다.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# optim 패키지를 사용하여 모델의 가중치를 갱신할 Optimizer를 정의합니다.\n",
    "# 여기서는 Adam을 사용하겠습니다; optim 패키지는 다른 다양한 최적화 알고리즘을\n",
    "# 포함하고 있습니다. Adam 생성자의 첫번째 인자는 어떤 Tensor가 갱신되어야 하는지\n",
    "# 알려줍니다.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계 전에, Optimizer 객체를 사용하여 (모델의 학습 가능한 가중치인)\n",
    "    # 갱신할 변수들에 대한 모든 변화도를 0으로 만듭니다. 이렇게 하는 이유는\n",
    "    # 기본적으로 .backward()를 호출할 때마다 변화도가 버퍼(buffer)에 (덮어쓰지 않고)\n",
    "    # 누적되기 때문입니다. 더 자세한 내용은 torch.autograd.backward에 대한 문서를\n",
    "    # 참조하세요.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 매개변수에 대한 손실의 변화도를 계산합니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer의 step 함수를 호출하면 매개변수가 갱신됩니다.\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3.33343505859375\n",
      "199 0.08401478081941605\n",
      "299 0.006234439555555582\n",
      "399 0.0006880204891785979\n",
      "499 9.524886991130188e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        생성자에서 2개의 nn.Linear 모듈을 생성하고, 멤버 변수로 지정합니다.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 함수에서는 입력 데이터의 Tensor를 받고 출력 데이터의 Tensor를\n",
    "        반환해야 합니다. Tensor 상의 임의의 연산자뿐만 아니라 생성자에서 정의한\n",
    "        Module도 사용할 수 있습니다.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 앞에서 정의한 클래스를 생성하여 모델을 구성합니다.\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# 손실 함수와 Optimizer를 만듭니다. SGD 생성자에 model.parameters()를 호출하면\n",
    "# 모델의 멤버인 2개의 nn.Linear 모듈의 학습 가능한 매개변수들이 포함됩니다.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 36.54949951171875\n",
      "199 6.915027141571045\n",
      "299 0.5637561678886414\n",
      "399 0.9523078203201294\n",
      "499 0.8864928483963013\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        생성자에서 순전파 단계에서 사용할 3개의 nn.Linear 인스턴스를 생성합니다.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        모델의 순전파 단계에서, 무작위로 0, 1, 2 또는 3 중에 하나를 선택하고\n",
    "        은닉층을 계산하기 위해 여러번 사용한 middle_linear Module을 재사용합니다.\n",
    "\n",
    "        각 순전파 단계는 동적 연산 그래프를 구성하기 때문에, 모델의 순전파 단계를\n",
    "        정의할 때 반복문이나 조건문과 같은 일반적인 Python 제어 흐름 연산자를 사용할\n",
    "        수 있습니다.\n",
    "\n",
    "        여기에서 연산 그래프를 정의할 때 동일 Module을 여러번 재사용하는 것이\n",
    "        완벽히 안전하다는 것을 알 수 있습니다. 이것이 각 Module을 한 번씩만 사용할\n",
    "        수 있었던 Lua Torch보다 크게 개선된 부분입니다.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 앞서 정의한 클래스를 생성(instantiating)하여 모델을 구성합니다.\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# 손실함수와 Optimizer를 만듭니다. 이 이상한 모델을 순수한 확률적 경사 하강법\n",
    "# (stochastic gradient decent)으로 학습하는 것은 어려우므로, 모멘텀(momentum)을\n",
    "# 사용합니다.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
