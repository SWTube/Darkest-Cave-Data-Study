{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"케라스와 텐서플로 허브를 사용한 영화 리뷰 텍스트 분류하기.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNWpGlyosH6OOF9q+zY1wzF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"gcbwx2s-YUw1"},"source":["import numpy as np\n","\n","import tensorflow as tf\n","\n","!pip install -q tensorflow-hub\n","!pip install -q tfds-nightly\n","import tensorflow_hub as hub\n","import tensorflow_datasets as tfds\n","\n","print(\"버전: \", tf.__version__)\n","print(\"즉시 실행 모드: \", tf.executing_eagerly())\n","print(\"허브 버전: \", hub.__version__)\n","print(\"GPU\", \"사용 가능\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"사용 불가능\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m8cZRyKqYikH"},"source":["# 훈련 세트를 6대 4로 나눕니다.\n","# 결국 훈련에 15,000개 샘플, 검증에 10,000개 샘플, 테스트에 25,000개 샘플을 사용하게 됩니다.\n","train_data, validation_data, test_data = tfds.load(\n","    name=\"imdb_reviews\", \n","    split=('train[:60%]', 'train[60%:]', 'test'),\n","    as_supervised=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bwbeyPSzYl6a"},"source":["## 데이터 탐색\n","\n","이 데이터셋의 샘플은 전처리된 정수 배열이다. 이 정수는 영화 리뷰에 나오는 단어를 나타낸다. 레이블은 정수 0 또는 1이다. 0은 부정 1은 긍정이다."]},{"cell_type":"code","metadata":{"id":"vopOvKk6Y80F"},"source":["train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n","train_examples_batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s514CBXUY_kg"},"source":["train_labels_batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tsvH4DJ1ZBMS"},"source":["## 모델 구성\n","\n","신경망은 층을 쌓아서 만든다. 여기에는 3개의 중요한 구조적 결정이 필요하다.\n","\n","> 어떻게 텍스트를 표현할 것인가\n","\n","> 모델에서 얼마나 많은 층을 사용할 것인가\n","\n","> 각 층에서 얼마나 많은 *은닉유닛*을 사용할 것인가\n","\n","텍스트를 표현하는 한 가지 방법은 문장을 임베딩 벡터로 바꾸는 것이다. 그러면 첫번째 층으로 사전훈련된 텍스트 임베딩을 사용할 수 있다. 이것의 장점은 \n","\n","> 텍스트 전처리에 신경 쓸 필요가 없다.\n","\n","> 전이 학습의 장점을 이용한다.\n","\n","> 임베딩은 고정 크기이기 때문에 처리 과정이 단순해진다."]},{"cell_type":"code","metadata":{"id":"MyzPk0p6b_S5"},"source":["embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n","hub_layer = hub.KerasLayer(embedding, input_shape=[], \n","                           dtype=tf.string, trainable=True)\n","hub_layer(train_examples_batch[:3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9hSesDAcSX8"},"source":["model = tf.keras.Sequential()\n","model.add(hub_layer)\n","model.add(tf.keras.layers.Dense(16, activation='relu'))\n","model.add(tf.keras.layers.Dense(1))\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtrjiodKcUBV"},"source":["1. 첫번째 층은 텐서플로 허브 층이다. 이 층은 사전 훈련된 모델을 사용하여 하나의 문장을 임베딩 벡터로 매핑한다. 여기서 사용하는 사전 훈련된 텍스트 임베딩 모델은 하나의 문장을 토큰으로 나누고 각 토큰의 임베딩을 연결하여 반환한다. 최종 차원은 (num_examples, embedding_demension)이다.\n","\n","2. 이 고정 크기의 출력 벡터는 16개의 은닉 유닛을 가진 완전 연결층(Dense)로 주입된다.\n","\n","3. 마지막 층은 하나의 출력 노드를 가진 완전 연결 층이다. sigmoid 활성화 함수를 사용하므로 확률 또는 신뢰도 수준을 표현하는 0~1 사이의 실수가 출력된다.\n","\n","## 손실함수와 옵티마이저 \n","\n","모델이 훈련하려면 손실함수와 옵티마이저가 필요하다. 지금은 이진 분류문제이고 확률을 출력하므로 binary_crossentropy 손실 함수를 사용한다."]},{"cell_type":"code","metadata":{"id":"hNtJiknjciod"},"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kAsbJlFjckiW"},"source":["## 모델훈련"]},{"cell_type":"code","metadata":{"id":"G-UfSlDLcnUg"},"source":["history = model.fit(train_data.shuffle(10000).batch(512),\n","                    epochs=20,\n","                    validation_data=validation_data.batch(512),\n","                    verbose=1)"],"execution_count":null,"outputs":[]}]}