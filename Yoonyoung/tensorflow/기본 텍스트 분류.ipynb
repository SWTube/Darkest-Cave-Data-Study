{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"기본 텍스트 분류.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOs2JfqjbmmU3avxEeiG9/G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"itSEeZn8Hi0s"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","import numpy as np\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tf9eP59vHpeZ"},"source":["## IMDB 데이터셋 다운로드"]},{"cell_type":"code","metadata":{"id":"u6gDqCTmHuS7"},"source":["imdb = keras.datasets.imdb\n","\n","(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W1ciOf8jHyKN"},"source":["매개변수 num_words=10000은 훈련 데이터에서 가장 많이 등장하는 상위 10000개의 단어를 선택한다.\n","\n","## 데이터 탐색\n","\n","이 데이터셋의 샘플은 전처리된 정수 배열이다. 이 정수는 영화 리뷰에 나오는 단어를 나타낸다. 레이블은 정수 0 또는 1이다. 0은 부정적인 리뷰 1은 긍정적인 리뷰"]},{"cell_type":"code","metadata":{"id":"55w_C9xxH3GH"},"source":["print(\"훈련 샘플: {}, 레이블: {}\".format(len(train_data), len(train_labels)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jsY1BGHyIIhc"},"source":["print(train_data[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2wDvQY7jIKeW"},"source":["len(train_data[0]), len(train_data[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ODzVD8WNINPv"},"source":["## 정수를 단어로 다시 변환\n","\n","정수와 문자열을 매핑하는 딕셔너리 객체에 질의하는 helper 함수를 만든다. "]},{"cell_type":"code","metadata":{"id":"6NvX2KDLIV3T"},"source":["# 단어와 정수 인덱스를 매핑한 딕셔너리\n","word_index = imdb.get_word_index()\n","\n","# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n","word_index = {k:(v+3) for k,v in word_index.items()}\n","word_index[\"<PAD>\"] = 0\n","word_index[\"<START>\"] = 1\n","word_index[\"<UNK>\"] = 2  # unknown\n","word_index[\"<UNUSED>\"] = 3\n","\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","\n","def decode_review(text):\n","    return ' '.join([reverse_word_index.get(i, '?') for i in text])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"84ook-AnJuOH"},"source":["decode_review(train_data[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"inpszhRUJyTq"},"source":["## 데이터 준비\n","\n","리뷰 -정수배열-는 신경망에 주입하기 전에 텐서로 변환되어야 한다.\n","\n","턴서 변환 방법\n","\n","> 원-핫 인코딩은 정수 배열을 0과 1로 이루어진 벡터로 변환한다. 그 다음 실수 벡터 데이터를 다룰수 있는 Dense층을 신경망의 첫번째 층으로 사용한다. 이 방법은 num_words * num_reviews 크기의 행렬이 필요하기 때문에 메모리를 많이 사용한다.\n","\n","> 정수 배열의 길이가 모두 같도록 패딩을 추가해서 max_length * num_reviews 크기의 정수 텐서를 만든다. 이 형태의 텐서를 다룰 수 있는 임베딩 층을 첫번째 층으로 사용할 수 있다.\n","\n","여기선 2번째 방법을 사용한다.\n","\n","영화 리뷰의 길이가 같아야하므로 pad_sequences함수를 사용한다."]},{"cell_type":"code","metadata":{"id":"5V5zsARNKUxy"},"source":["train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n","                                                        value=word_index[\"<PAD>\"],\n","                                                        padding='post',\n","                                                        maxlen=256)\n","\n","test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n","                                                       value=word_index[\"<PAD>\"],\n","                                                       padding='post',\n","                                                       maxlen=256)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lBWJeR-LKhxo"},"source":["len(train_data[0]), len(train_data[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gm_4dIVcElpI"},"source":["# 패딩된 첫 리뷰 내용\n","print(train_data[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UxitQJUjEyTs"},"source":["## 모델 구성\n","신경망은 layer를 쌓아서 만든다. 이 때 두가지를 결정해야 한다.\n","\n","> 모델에서 얼마나 많은 *층*을 사용할 것인가?\n","\n","> 각 층에서 얼마나 많은 *은닉 유닛*을 사용할 것인가?"]},{"cell_type":"code","metadata":{"id":"OJYDh0-mExwV"},"source":["# 입력 크기는 영화 리뷰 데이터셋에 적용된 어휘 사전의 크기입니다(10,000개의 단어)\n","vocab_size = 10000\n","\n","model = keras.Sequential()\n","model.add(keras.layers.Embedding(vocab_size, 16, input_shape=(None,)))\n","model.add(keras.layers.GlobalAveragePooling1D())\n","model.add(keras.layers.Dense(16, activation='relu'))\n","model.add(keras.layers.Dense(1, activation='sigmoid'))\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cU18SAyEFp64"},"source":["1. 첫번째 층은 embeding 층이다. 이 층은 정수로 인코딩된 단어를 입력받고 각 단어 인덱스에 해당하는 임베딩 벡터를 찾는다. 이 벡터는 모델이 훈련되면서 학습된다. 이 벡터는 출력 배열에 새로운 차원으로 추가된다. 최종 차원은 (batchm sequence, embedding)\n","embedding은 자연어를 수치화 하기위해 필요한 layer\n","\n","2. 그 다음 GlobalAveragePooling1D 층은 sequence 차원에 대해 평균을 계산하여 각 샘플에 대해 고정된 길이의 출력 벡터를 반환한다. 이는 길이가 다른 입력을 다루는 가장 간단한 방법이다.\n","\n","3. 이 고정 길이의 출력 벡터는 16개의 은닉 유닛을 가진 Dense를 거친다.\n","\n","4. 마지막 층은 하나의 출력 노드를 가진 Dense층이다. sigmoid 활성화 함수를 사용하여 0과 1사이의 실수를 출력한다. 이 값은 확률 또는 신뢰도를 나타낸다. \\\n","\n","## 은닉 유닛\n","\n","위 모델에는 입력과 출력 사이에 두개의 \"은닉\"층이 있다. 출력의 개수는 층이 가진 표현 공간의 차원이 된다. 다른 말로 하면 내부 표현을 학습할 때 허용되는 네트워크 자유도의 양이다.\n","\n","모델에 많은 은닉 유닛과 층이 있다면 네트워크는 더 복잡한 표현을 학습할 수 있다. 하지만 네트워크의 계산 비용이 ㅁ낳이 들고 원치 않는 패턴을 학습할 수도 있다. 이런 표현은 훈련 데이터의 성능을 향상시키지만 테스트 데이터에서는 그렇지 못한다. 이를 *과대적합*이라고 부른다. \n","\n","## 손실 함수와 옵티마이저\n","\n","모델이 훈련하기 위해 필요한 것이 손실함수와 옵티마이저이다. "]},{"cell_type":"code","metadata":{"id":"IPqeqYk8FEoG"},"source":["model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dLxR2kjASZ3G"},"source":["## 검증 세트 만들기\n","\n","모델을 훈련할 때 모델이 만난 적 없는 데이터에서 정확도를 확인하는 것이 좋다. 원본 데이터에서 10,000개의 샘플을 떼어내어 검증세트를 만든다. "]},{"cell_type":"code","metadata":{"id":"ph2aD2-ySmfC"},"source":["x_val = train_data[:10000]\n","partial_x_train = train_data[10000:]\n","\n","y_val = train_labels[:10000]\n","partial_y_train = train_labels[10000:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y7ZH7xpYSoI6"},"source":["## 모델 훈련\n","\n","이 보델을 312개의 샘플로 이루어진 미니 배치에서 40번의 에포크동안 훈련한다. x_train과 y_train 텐서에 있는 모든 샘플에 대해 40번 반복한다."]},{"cell_type":"code","metadata":{"id":"CL2yX3f1SwO5"},"source":["history = model.fit(partial_x_train,\n","                    partial_y_train,\n","                    epochs=40,\n","                    batch_size=512,\n","                    validation_data=(x_val, y_val),\n","                    verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5nlBeGlITDPL"},"source":["## 모델 평가\n","\n","모델의 성능은 손실과 정확도로 평가한다."]},{"cell_type":"code","metadata":{"id":"MTz8RZxkTHps"},"source":["results = model.evaluate(test_data,  test_labels, verbose=2)\n","\n","print(results)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1mdNU-SDTX2K"},"source":["## 정확도와 손실 그래프\n","\n","model.fit()은 history 객체를 반환한다. 여기에는 훈련하는 동안 일어나는 모든 정보가 dictionary에 들어가있다."]},{"cell_type":"code","metadata":{"id":"6x81btXFT49h"},"source":["history_dict = history.history\n","history_dict.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4LsXRcqsT2Lh"},"source":["import matplotlib.pyplot as plt\n","\n","acc = history_dict['accuracy']\n","val_acc = history_dict['val_accuracy']\n","loss = history_dict['loss']\n","val_loss = history_dict['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","# \"bo\"는 \"파란색 점\"입니다\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","# b는 \"파란 실선\"입니다\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zOQLqPMOT6-W"},"source":["plt.clf()   # 그림을 초기화합니다\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MK0wcA2sUHaq"},"source":["점선은 훈련 손실과 훈련 정확도, 실선은 검증 손실과 검증 정확도이다.\n","\n","훈련 손실은 에포크마다 감소하고 훈련 정확도는 증가한다. 경사 하강법 최적화를 사용할 때마다 볼 수 있는 현상이다. 매 반복마다 최적화 대상의 값을 최소화한다.\n","\n","그러나 검증 손실과 검증 정확도에서는 그렇지 못하다. 약 20번쨰 에포크 이후 최적짐인 듯 하다. 이는 *과대적합* 때문이다. 이전에 본 적 없는 데이터보다 훈련 데이터에서 잘 작동한다. 이 지점부터 모델이 과도하게 최적화되어 테스트 데이터에서 일반화되기 어려운 훈련데이터의 특징 표현을 학습한다."]}]}