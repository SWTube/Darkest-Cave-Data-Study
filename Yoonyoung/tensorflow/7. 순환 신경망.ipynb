{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7. 순환 신경망.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO8xUdbq2Ya9p93UJsS5DCu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qn-YWQd_n_iC"},"source":["# 순환 신경망\r\n","\r\n","순환 신경망은 순서가 있는 데이터를 입력으로 받고 같은 네트워크를 이용해 변화하는 입력에 대한 출력을 얻어낸다. 시간의 흐름에 따라 변화하고 그 변화가 의미를 갖는 데이터이다. \r\n","\r\n","## 7.1 순환 신경망의 구조\r\n","\r\n","출력값이 다음 입력을 받을 때의 RNN 네트워크에도 동일하게 전달된다. \r\n","\r\n","순환 신경망은 입력과 출력의 길이에 제한이 없다. 그렇기 때문에 다양한 형태의 네트워크를 만들 수 있다. \r\n","\r\n","## 7.2 주요 레이어 정리\r\n","\r\n","### SImpleRNN 레이어\r\n","활성화함수로는 tanh가 쓰인다. tanh는 -1에서 1 사이의 출력값을 반환한다. 이 자리에 ReLU같은 다른 활성화함수도 쓸 수 있다.\r\n","\r\n","    rnn1 = tf.keras.layers.SimpleRNN(units=1, activation='tanh', return_sequences=True)\r\n","\r\n","units는 SimpleRNN레이어에 존재하는 뉴런의 수.\r\n","\r\n","return_sequences는 출력으로 시퀀스 전체를 출력할지 여부를 나타내는 옵션, 주로 여러개의 RNN 레이어를 쌓을때 쓴다.\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"gBjpmisW1D19"},"source":["import numpy as np\r\n","# 0.0, 0.1, 0.2, 0.3을 통해 0.4를 예측하는 네트워크를 만들어보자!\r\n","\r\n","X = [] \r\n","Y = []\r\n","for i in range(6):\r\n","    # [0, 1, 2, 3], [1, 2, 3, 4]와 같은 정수의 시퀀스를 만든다\r\n","    lst = list(range(i, i+4))\r\n","\r\n","    # 위에서 구한 시퀀스의 숫자들을 각각 10으로 나눈 다음 저장한다.\r\n","    # SimpleRNN에 각 타임스텝에 하나씩 숫자가 들어가기 때문에 여기서도 하나씩 분리해서 배열에 저장한다.\r\n","    X.append(list(map(lambda c: [c/10], lst)))\r\n","\r\n","    #정답에 해당하는 4, 5 등의 정수 역시 앞에서처럼 10으로 나눠서 저장한다.\r\n","    Y.append((i+4)/10)\r\n","\r\n","X = np.array(X)\r\n","Y = np.array(Y)\r\n","for i in range(len(X)):\r\n","    print(X[i], Y[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xeebdfNybM3A"},"source":["import tensorflow as tf\r\n","\r\n","model = tf.keras.Sequential([\r\n","                             tf.keras.layers.SimpleRNN(units=10, return_sequences=False, input_shape = [4,1]),\r\n","                             tf.keras.layers.Dense(1)\r\n","])\r\n","\r\n","model.compile(optimizer='adam', loss='mse')\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YAU0T2eLbluW"},"source":["SimpleRNN 레이어에서 주목해야할 점은 **input_shape**이다. 여기서 [4,1]은 각각 timesteps, input_dim을 나타낸다. \r\n","\r\n","timesteps는 순환 신경망이 입력에 대해 계산을 반복하는 횟수이고\r\n","\r\n","input_dim은 입력 벡터의 크기이다."]},{"cell_type":"code","metadata":{"id":"cUw5B3-DcJat"},"source":["model.fit(X,Y, epochs=100, verbose=0)\r\n","print(model.predict(X))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8jlolDpcYgZ"},"source":["print(model.predict(np.array([[[0.6], [0.7], [0.8], [0.9]]])))\r\n","print(model.predict(np.array([[[-0.1], [0.0], [0.1], [0.2]]])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LBjhgyw8ctb5"},"source":["### LSTM 레이어\r\n","\r\n","SimpleRNN 레이어의 치명적인 단점은 입력 데이터가 길어질수록 학습 능력이 떨어진다. \r\n","\r\n","입력 데이터와 출력 사이의 길이가 멀어질수록 연관 관계가 작아진다는 것이다. \r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"-AiUwdSCdrSC"},"source":["# 예시) 곱셈문제\r\n","\r\n","X = []\r\n","Y = []\r\n","for i in range(3000):\r\n","# 0 ~ 1 범위의 랜덤한 숫자 100개를 만든다.\r\n","    lst = np.random.rand(100)\r\n","\r\n","    #마킹할 숫자 2개의 인덱스를 뽑는다.\r\n","    idx = np.random.choice(100, 2, replace=False)\r\n","    \r\n","    #마킹 인덱스가 저장된 원-핫 인코딩 벡터를 만든다\r\n","    zeros = np.zeros(100)\r\n","    zeros[idx] = 1\r\n","    \r\n","    # 마킹 인덱스와 랜덤한 숫자를 합쳐서 X에 저장한다.\r\n","    X.append(np.array(list(zip(zeros, lst))))\r\n","\r\n","    # 마킹 인덱스가 1인 값만 서로 곱해서 Y에 저장한다.]\r\n","    Y.append(np.prod(lst[idx]))\r\n","\r\n","print(X[0], Y[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bgshOxtdfn-y"},"source":["model = tf.keras.Sequential([\r\n","                             tf.keras.layers.SimpleRNN(units=30, return_sequences=True, input_shape=[100,2]),\r\n","                             tf.keras.layers.SimpleRNN(units=30),\r\n","                             tf.keras.layers.Dense(1)\r\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9bT7e7LgAb6"},"source":["위 모델에서 return_sequences = True로 되어있다.  return_sequences는 레이어의 출력ㅇ르 다음 레이어로 그대로 넘겨주게 된다."]},{"cell_type":"code","metadata":{"id":"76L_HzOWf3mn"},"source":["model.compile(optimizer='adam', loss='mse')\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gSy0kF-jkWrs"},"source":["X = np.array(X)\r\n","Y = np.array(Y)\r\n","\r\n","# 2560 개의 데이터만 학습시키나. 검증 데이터는 20%로 지정\r\n","history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split = 0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nF9gVK7lr3I"},"source":["import matplotlib.pyplot as plt\r\n","\r\n","plt.plot(history.history['loss'], 'b-', label='loss')\r\n","plt.plot(history.history['val_loss'], 'r--', label='val_loss')\r\n","plt.legend()\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uA9fBPvNntFX"},"source":["model.evaluate(X[2560:], Y[2560:])\r\n","prediction = model.predict(X[2560:2560+5])\r\n","\r\n","for i in range(5):\r\n","    print(Y[2560 + i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\r\n","\r\n","prediction = model.predict(X[2560:])\r\n","fail = 0\r\n","for i in range(len(prediction)):\r\n","    if abs(prediction[i][0] - Y[2560+i]) > 0.04:\r\n","        fail += 1\r\n","\r\n","print('correctness:', (440 - fail) / 440 * 100, '%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nbUDdIrbqEf0"},"source":["SimpleRNN으로 만들어 보았을 때 전체적인 loss는 0.0484가 나왔다. 위에서 본 100번째 에포크의 val_loss인 0.0474보다 높으므로 네트워크가 학습과정에서 한번도 못 본 테스트 데이터에 대해서는 예측을 잘 하지 못한다. 5개의 테스트 데이터에 대한 정확도가 10.9%으로 나온다.\r\n","\r\n","그렇다면 LSTM은 어떨까?\r\n"]},{"cell_type":"code","metadata":{"id":"ju4W5A6fqptS"},"source":["model = tf.keras.Sequential([\r\n","                             tf.keras.layers.LSTM(units=30, return_sequences=True, input_shape=[100,2]),\r\n","                             tf.keras.layers.LSTM(units=30),\r\n","                             tf.keras.layers.Dense(1)\r\n","])\r\n","\r\n","model.compile(optimizer='adam', loss='mse')\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p3fPpbeHrL1P"},"source":["X = np.array(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cCqwiy5wrOdh"},"source":["Y = np.array(Y)\r\n","history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdgO4XzuwfWg"},"source":["import matplotlib.pyplot as plt\r\n","\r\n","plt.plot(history.history['loss'], 'b-', label='loss')\r\n","plt.plot(history.history['val_loss'], 'r--', label='val_loss')\r\n","plt.legend()\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"COkcHA0BxCtV"},"source":["model.evaluate(X[2560:], Y[2560:])\r\n","prediction = model.predict(X[2560:2560+5])\r\n","\r\n","for i in range(5):\r\n","    print(Y[2560 + i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\r\n","\r\n","prediction = model.predict(X[2560:])\r\n","fail = 0\r\n","for i in range(len(prediction)):\r\n","    if abs(prediction[i][0] - Y[2560+i]) > 0.04:\r\n","        fail += 1\r\n","\r\n","print('correctness:', (440 - fail) / 440 * 100, '%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKwAXtOnxRme"},"source":["LSTM이 훨씬 높은 정확도 92,5%를 보이고 loss도 0.04보다 높은게 1도 없음\r\n","\r\n","### GRU 레이어\r\n","\r\n","LSTM 레이어와 비슷한 역할을 하지만 계산 구조가 훨씬 간단.\r\n","\r\n","앞서 했던 문제를 이번엔 GRU 레이어로 풀어보겠음"]},{"cell_type":"code","metadata":{"id":"DDvPjzPZyLdB"},"source":["model = tf.keras.Sequential([\r\n","                             tf.keras.layers.GRU(units=30, return_sequences=True, input_shape = [100,2]),\r\n","                             tf.keras.layers.GRU(units=30),\r\n","                             tf.keras.layers.Dense(1)\r\n","])\r\n","\r\n","model.compile(optimizer='adam', loss='mse')\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xNn3J0dQyipq"},"source":["위 곱셈 문제를 풀기 위한 네트워크 파라미터 수\r\n","\r\n","SimpleRNN 2851개\r\n","LSTM 11311개\r\n","GRU 8671개개"]},{"cell_type":"code","metadata":{"id":"g5L5BzX-zB3W"},"source":["X = np.array(X)\r\n","Y = np.array(Y)\r\n","history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZiSvAtMo3SdV"},"source":["import matplotlib.pyplot as plt\r\n","\r\n","plt.plot(history.history['loss'], 'b-', label='loss')\r\n","plt.plot(history.history['val_loss'], 'r--', label='val_loss')\r\n","plt.legend()\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6vYNRfd34P_"},"source":["model.evaluate(X[2560:], Y[2560:])\r\n","prediction = model.predict(X[2560:2560+5])\r\n","\r\n","for i in range(5):\r\n","    print(Y[2560 + i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\r\n","\r\n","prediction = model.predict(X[2560:])\r\n","cnt = 0\r\n","for i in range(len(prediction)):\r\n","    if abs(prediction[i][0] - Y[2560+i]) > 0.04:\r\n","        cnt += 1\r\n","\r\n","print('correctness:', (440 - cnt) / 440 * 100, '%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKLre9bX4Nwj"},"source":["### 임베딩 레이어\r\n","\r\n","임베딩 레이어는 자연어를 수치화 정보로 바꾸기 위한 레이어이다.\r\n","\r\n","임베딩 레이어보다 좀 더 쉬운 기법은 자연어를 구성하는 단위에 대해 정수 인덱스를 저장하는 방법이다. 단어를 기반으로 정수 인덱스를 저장하여 사용하는 것이다. 그 다음 이를 원-핫 인코딩을 이요하여 0과 1로만 이루어진 열로 바꾸어준다.\r\n","\r\n","인덱스를 사용하는 원-핫 인코딩 방식의 단점은 메모리의 양에 비해 너무 적은 정보량을 사용한다는 것이다. 또한 인덱스에 저장된 단어의 수가 많아질수록 원-핫 인코딩 배열의 두번째 차원의 크기도 비례해서 늘어난다는 것이다.\r\n","\r\n","원-핫 인코딩 방식에 비해 임베딩 레이어는 한정된 길이의 벡터로 자연어의 구성 단위인 자소, 문자, 단어, ngram 등을 표현할 수 있다."]},{"cell_type":"markdown","metadata":{"id":"dIH5j2Cd7q2D"},"source":["## 7.3 긍정, 부정 감성 분석"]},{"cell_type":"code","metadata":{"id":"odtpFlQ47u9p"},"source":["import tensorflow as tf\r\n","import numpy as np\r\n","    \r\n","\r\n","path_to_train_file = tf.keras.utils.get_file('train.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\r\n","path_to_test_file = tf.keras.utils.get_file('test.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MAJfBWdn8Lzn"},"source":["# 데이터를 메모리에 불러온다. 인코딩 형식으로 utf-8을 지정해야한다.\r\n","train_text = open(path_to_train_file, 'rb').read().decode(encoding='utf-8')\r\n","test_text = open(path_to_test_file, 'rb').read().decode(encoding='utf-8')\r\n","\r\n","# 텍스트가 총 몇자인지 확인한다.\r\n","\r\n","print('train text 길이 : {}'.format(len(train_text)))\r\n","print('test text 길이 : {}'.format(len(test_text)))\r\n","print() \r\n","\r\n","print(train_text[:300])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0450uVtP94Rd"},"source":["데이터의 각 행은 '\\t'으로 구분되어 있다. 각 id는 각 데이터의 고유한 id 이고 document는 실제 리뷰 내용이다. label은 긍.부정 내용이다."]},{"cell_type":"code","metadata":{"id":"XGa8WD-r9oqc"},"source":["train_Y = np.array([[int(row.split('\\t')[2])] for row in train_text.split('\\n')[1:] if row.count('\\t') > 0])\r\n","test_Y = np.array([[int(row.split('\\t')[2])] for row in test_text.split('\\n')[1:] if row.count('\\t') > 0])\r\n","\r\n","print(train_Y.shape, test_Y.shape)\r\n","print(train_Y[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"il1uvvv--nVu"},"source":["그 다음 입력으로 쓸 자연어를 토큰화하고 정제해야한다. 토큰화로는 단어를 사용할 것이기 때문에 띄어쓰기를 단위로 나누면 된다. 정제란 원하지 않는 입력이나 불필요한 기호 등ㅇ르 제거하는 것이다. 정제를 위해 함수로는 김윤 박사의 CNN_sentence 깃허브 저장소의 코드를 사용한다. "]},{"cell_type":"code","metadata":{"id":"Rjoyf9tB-3lW"},"source":["import re\r\n","# From https://github.com/yoonkim/CNN_sentence/blob/master/process_daata.py\r\n","\r\n","def clean_str(string):\r\n","    \"\"\"\r\n","    Tokenization/string cleaning for all datasets except for SST.\r\n","    Every dataset is lower cased except for TREC\r\n","    \"\"\"\r\n","    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \r\n","    string = re.sub(r\"\\'s\", \" \\'s\", string) \r\n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \r\n","    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \r\n","    string = re.sub(r\"\\'re\", \" \\'re\", string) \r\n","    string = re.sub(r\"\\'d\", \" \\'d\", string) \r\n","    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \r\n","    string = re.sub(r\",\", \" , \", string) \r\n","    string = re.sub(r\"!\", \" ! \", string) \r\n","    string = re.sub(r\"\\(\", \" \\( \", string) \r\n","    string = re.sub(r\"\\)\", \" \\) \", string) \r\n","    string = re.sub(r\"\\?\", \" \\? \", string) \r\n","    string = re.sub(r\"\\s{2,}\", \" \", string)    \r\n","    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\r\n","    string = re.sub(r\"\\'\", \"\", string)\r\n","\r\n","    return string.lower()\r\n","\r\n","train_text_X = [row.split('\\t')[1] for row in train_text.split('\\n')[1:] if row.count('\\t') > 0]\r\n","train_text_X =[clean_str(sentence) for sentence in train_text_X]\r\n","# 문장을 띄어쓰기 단위로 단어 분리\r\n","sentences = [sentence.split(' ') for sentence in train_text_X]\r\n","for i in range(5):\r\n","    print(sentences[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"On1kbFlCDKUd"},"source":["    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \r\n","\r\n","다음 첫 줄의 의미는 한글, 영어, 숫자, 괄호, 쉼표, 느낌표, 물음표, 작은 따움표, 역 따옴표를 제외한 모든 것ㅇ르 공백으로 바꾸겠다는 의미.\r\n","\r\n","훈련 데이터의 처음 다섯개를 출력해보면 구두점(.)같은 기호가 삭제된 것을 알 수 있다.\r\n","\r\n","하지만 네트워크에 입력하려면 데이터의 크기가 동일해야한다. 하지만 현재의 각 문장의 길이가 다르기 때문에 문장의 길이를 맞춰야 한다. 이를 위해 적당한 길이의 문장이 어느정도인지 확인하고 긴 문장은 줄이고 짧은 문장에는 공백을 넣는 패딩을 채운다. "]},{"cell_type":"code","metadata":{"id":"LBBaNSAiDy19"},"source":["import matplotlib.pyplot as plt\r\n","sentence_len = [len(sentence) for sentence in sentences]\r\n","sentence_len.sort()\r\n","plt.plot(sentence_len)\r\n","plt.show()\r\n","\r\n","print(sum([int(l<=25) for l in sentence_len]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uz1jeYa8EGSM"},"source":["y축이 문장의 단어 개수일 때 15만개의 문장 중에서 대부분 40단어 이하로 구성되어있다.  특히 25단어 이하인 문장의 수는 142,587개로 전체의 95%정도이다. \r\n","\r\n","따라서 기분이 되는 문장의 길이를 25개로 잡고 이 이상은 생략, 이 이하는 패딩으로 25로 맞춰주면 임베딩 레이어에 넣을 준비가 끝난다.\r\n","\r\n","또 하나 처리해야하는 부분은 각 단어의 최대 길이를 조정하는 일이다.\r\n"]},{"cell_type":"code","metadata":{"id":"T0M7g6vyEcHd"},"source":["sentences_new = []\r\n","for sentence in sentences:\r\n","    sentences_new.append([word[:5] for word in sentence][:25])\r\n","sentences = sentences_new\r\n","for i in range(5):\r\n","    print(sentences[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZCqOSi0cE0q3"},"source":["이제는 한 문장의 길이를 25으로 바꾸기 위해서 tf.keras에서 제공하는 pad_sequences를 사용한다."]},{"cell_type":"code","metadata":{"id":"rkp9PomzE6YT"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","\r\n","tokenizer = Tokenizer(num_words=20000)\r\n","tokenizer.fit_on_texts(sentences)\r\n","train_X = tokenizer.texts_to_sequences(sentences)\r\n","train_X = pad_sequences(train_X, padding='post')\r\n","\r\n","print(train_X[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FrLVBRtzFaWo"},"source":["Tokenizer는 데이터에 출현하는 모든 단어의 개수를 세고 빈도수를 정렬해서  num_words에 지정된 만큼만 숫자로 반환하고 나머지는 0으로 반환한다.\r\n","\r\n","tokenizer.fit_on_texts(sentences)는 tokenizer에 데이터를 실제로 입력한다.\r\n","\r\n","이 과정을 거친 뒤 tokenizer.texts_to_sequences(sentences)는 문장을 입력받아 숫자를 반환한다. 마지막으로 pad_sequences()는 입력된 데이터에 패딩을 더한다.\r\n","\r\n","pad_sequences()의 padding 인수는 2가지 옵션이 있다. 이 중 'pre'는 문장 앞에 패딩을 넣고 'post'는 문장 뒤에 패딩을 넣는다.\r\n","\r\n","단어가 빈도수에서 상위 20,000개 안에 들지 못하면 0으로 처리된다. "]},{"cell_type":"code","metadata":{"id":"udu0W86-yMcu"},"source":["#Tokenizer 예시\r\n","print(tokenizer.index_word[19999])\r\n","print(tokenizer.index_word[20000])\r\n","temp = tokenizer.texts_to_sequences(['#$#$#', '경우는', '잊혀질', '연기가'])\r\n","print(temp)\r\n","temp = pad_sequences(temp, padding='post')\r\n","print(temp)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GDFpusqbysXK"},"source":["위에서  Tokenizer.index_word에 저장돼 있는 19,999번째 단어와 20,000번째 단어를 확인해본 뒤, 이 단어들로 구성된 문장을  Tokenizer에 넣어보았다.\r\n","\r\n","texts_to_sequences()에서 반환하는 데이터는 19,999번째 단어인 \"겨우는\"과 \"연기가\"만 의미 있는 데이터로 남고 나머지는 공백으로 반환한다.\r\n","\r\n","이 공백은 pad_sequences()를 통과하면서 0으로 바뀐다.\r\n","\r\n","여기서 pad_sequences()의 maxlen인수가 지정되지 않았기 때문에 입력된 문장 전체 길이 중 가장 긴 길이로 문장의 길이를 맞춘다. 입력 문장이 하나이기 때문에 입력과 출력 문장의 길이는 동일하다.\r\n","\r\n","실제로 네트워크를 정의하고 학습시켜보자.\r\n","\r\n","먼저 임베딩 레이어와 LSTM 레이어를 연결한 뒤 마지막에 Dense 레이어의 소프트맥스 활성화함수를 사용해 긍정/부정을 분류하는 네트워크를 정의해보자"]},{"cell_type":"code","metadata":{"id":"yyPW7ey7zZJQ"},"source":["model = tf.keras.Sequential([\r\n","                             tf.keras.layers.Embedding(20000, 300, input_length=25),\r\n","                             tf.keras.layers.LSTM(units=50),\r\n","                             tf.keras.layers.Dense(2, activation='softmax')\r\n","])\r\n","\r\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Fj_Dy5AzyuX"},"source":["위의 네트워크에서 임베딩 레이어는 시퀀셜 모델의 첫 레이어이기 때문에 입력 형태에 대한 정의가 필요하다. input_length 인수를 25로 지정해 각 문장에 들어있는 25개 단어를 길이 300의 임베딩 벡터로 변환한다.\r\n","\r\n","네트워크의 loss는 sparse_categorical_crosstropy를 사용했다. 여러 개의 정답 중 하나를 맞추는 분류에서는 categorical_crossentropy를 사용하고, sparse는 정답인 Y가 희소행렬일때 사용한다!"]},{"cell_type":"code","metadata":{"id":"FTmebxPS0K36"},"source":["history = model.fit(train_X, train_Y, epochs = 5, batch_size=128, validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k_TAlBg02sIb"},"source":["import matplotlib.pyplot as plt\r\n","plt.figure(figsize=(12,4))\r\n","\r\n","plt.subplot(1,2,1)\r\n","plt.plot(history.history['loss'], 'b-', label='loss')\r\n","plt.plot(history.history['val_loss'], 'r--', label='val_loss')\r\n","plt.legend()\r\n","\r\n","plt.subplot(1,2,2)\r\n","plt.plot(history.history['accuracy'], 'g-', label='accuracy')\r\n","plt.plot(history.history['val_accuracy'], 'k--', label='val_accuracy')\r\n","plt.legend()\r\n","\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8P73xk563m7n"},"source":["val_loss가 꾸준히 증가하고 val_accuracy도 점점 떨어지고 있다. == 과적합 되었따 \r\n","\r\n","과적합의 이유는 ㅇ미베딩 레이어를 랜덤한 값에서부터 시작해서 학습시키기 때문에 각 단어를 나타내는 벡터의 품질이 좋지 않아서이다.\r\n","\r\n","이를 개선하기 위한 방법으로 임베딩 레이어를 별도로 학습시켜서 네트워크에 불러와서 사용하거나, CNN을 사용하는 방법이 있다.\r\n","\r\n","학습된 네트워크가 테스트 데이터를 어떻게 평가하는지 확인하기 위해 test_text에도  train_text와 같은 변환 과정을 거쳐서 test_X를 만들고 model.evaluate()로 평가해 본다.\r\n","\r\n","이때 주목해야할 점은 train_X를 만들 때 학습시켰던 Tokenizer를 어떤 변경 없이 그대로 사용하고 있다는 것이다. 훈련 데이터와는 다르게 테스트 데이터에서는 어떤 단어가 나타날지 모르기 때문에 Tokenizer는 훈련 데이터로만 학습시켜야 한다. "]},{"cell_type":"code","metadata":{"id":"oeog7kEB4RXZ"},"source":["test_text_X = [row.split('\\t')[1] for row in test_text.split('\\n')[1:] if row.count('\\t') > 0]\r\n","test_text_X = [clean_str(sentence) for sentence in test_text_X]\r\n","sentences= [sentence.split(' ') for sentence in test_text_X]\r\n","sentences_new = []\r\n","\r\n","for sentence in sentences:\r\n","    sentences_new.append([word[:5] for word in sentence][:25])\r\n","sentences = sentences_new\r\n","\r\n","test_X = tokenizer.texts_to_sequences(sentences)\r\n","test_X = pad_sequences(test_X, padding='post')\r\n","\r\n","model.evaluate(test_X, test_Y, verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CbEpivl35cLw"},"source":["test_sentence = '재밌을 줄 알았는데 완전 실망했다. 너무 졸리고 돈이 아까웠다.'\r\n","test_sentence = test_sentence.split(' ')\r\n","test_sentences = []\r\n","now_sentence = []\r\n","for word in test_sentence:\r\n","    now_sentence.append(word)\r\n","    test_sentences.append(now_sentence[:])\r\n","\r\n","test_X_1 = tokenizer.texts_to_sequences(test_sentences)\r\n","test_X_1 = pad_sequences(test_X_1, padding='post', maxlen = 25)\r\n","prediction = model.predict(test_X_1)\r\n","for idx, sentence in enumerate(test_sentences):\r\n","    print(sentence)\r\n","    print(prediction[idx])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QI9zrXPl6WVE"},"source":["출력은 문장의 변화에 따른 감정 분석 예측 결과이다. 처음에는 \"재미있을\"이라는 단어만 입력됐을 때는 긍정의 확률이 높으나, 다른 단어들이 입력되면서 줄어들었다. "]},{"cell_type":"markdown","metadata":{"id":"kV6BQk9E6iGF"},"source":["## 자연어 생성\r\n","\r\n","### 단어 단위 생성"]},{"cell_type":"code","metadata":{"id":"MAWZmTMk6m8m"},"source":["path_to_file = tf.keras.utils.get_file('input.txt', 'http://bit.ly/2Mc3SOV')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zijHh0lL6z1G"},"source":["# 데이터를 메모리에 불러온다.\r\n","train_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\r\n","\r\n","# 텍스트가 총 몇 자인지 확인한다.\r\n","print('Length of text : {}'.format(len(train_text)))\r\n","print()\r\n","\r\n","#처음 1100자를 확인한다.\r\n","print(train_text[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-ikeeAt7NqT"},"source":["import re\r\n","# From https://github.com/yoonkim/CNN_sentence/blob/master/process_daata.py\r\n","\r\n","def clean_str(string):\r\n","    \"\"\"\r\n","    Tokenization/string cleaning for all datasets except for SST.\r\n","    Every dataset is lower cased except for TREC\r\n","    \"\"\"\r\n","    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \r\n","    string = re.sub(r\",\", \" , \", string) \r\n","    string = re.sub(r\"!\", \" ! \", string) \r\n","    string = re.sub(r\"\\(\", \"\", string) \r\n","    string = re.sub(r\"\\)\", \"\", string) \r\n","    string = re.sub(r\"\\?\", \" \\? \", string) \r\n","    string = re.sub(r\"\\s{2,}\", \" \", string)    \r\n","    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\r\n","    string = re.sub(r\"\\'\", \"\", string)\r\n","\r\n","    return string\r\n","\r\n","train_text = train_text.split('\\n')\r\n","train_text = [clean_str(sentence) for sentence in train_text]\r\n","train_text_X = []\r\n","for sentence in train_text:\r\n","    train_text_X.extend(sentence.split(' '))\r\n","    train_text_X.append('\\n')\r\n","\r\n","train_text_X = [word for word in train_text_X if word != '']\r\n","\r\n","print(train_text_X[:30])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"drvFl0gw-mfX"},"source":["다음으로 할 작업은 단어를 토큰화 하는 것이다. 여기서 직접 토큰화를 해보도록 하자."]},{"cell_type":"code","metadata":{"id":"hXebzAn6-q4J"},"source":["# 단어의 set을 만든다.\r\n","vocab = sorted(set(train_text_X))\r\n","vocab.append('UNK')\r\n","print(' {} unique words'.format(len(vocab)))\r\n","\r\n","# vocab list를 숫자로 매핑하고, 반대도 실행한다.\r\n","word2idx = {u:i for i,u in enumerate(vocab)}\r\n","idx2word = np.array(vocab)\r\n","\r\n","text_as_int = np.array([word2idx[c] for c in train_text_X])\r\n","\r\n","# word2idx 의 일부를 알아보기 쉽게 출력해보자.\r\n","print('{')\r\n","for word,_ in zip(word2idx, range(10)):\r\n","    print(' {:4s}: {:3d},'.format(repr(word), word2idx[word]))\r\n","print('...\\n}')\r\n","\r\n","print('index of UNK: {}'.format(word2idx['UNK']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4yTfudyz_ixL"},"source":["2번째 줄에서 텍스트에 들어간 단어가 중복되지 않는 리스트를 만든 다음, 3번째 줄에서 텍스트에 존재하지 않는 토큰을 나타내는 'UNK'를 넣는다. 총 단어수는 332,640이고, 이중 'UNK'의 인덱스는 332,639이다. 이 인덱스는 나중에 임의의 문장을 입력할 때 텍스트에 미리 준비돼 있지 않았던 단어를 쓸 수 있기 때문에 그에 대한 토큰으로 쓰일 것이다. "]},{"cell_type":"code","metadata":{"id":"E8yud1GZ_3tX"},"source":["print(train_text_X[:20])\r\n","print(text_as_int[:20])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gIBBTfHx__BW"},"source":["이제 학습을 위한 데이터 셋을 만든다. 여기서 기존의 train_X train_Y 를 넘파이 array로 만드는 방식이 아닌, tf.data.Dataset을 이용한다. Dataset의 장점은 간단한 코드로 데이터 섞기, 배치수 만큼 자르기, 다른 Dataset에 매핑하기 등을 수행할 수 있다는 점이다."]},{"cell_type":"code","metadata":{"id":"6Uf5QNln_-VC"},"source":["seq_length = 25\r\n","examples_per_epoch = len(text_as_int) // seq_length\r\n","sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\r\n","sentence_dataset = sentence_dataset.batch(seq_length+1, drop_remainder = True)\r\n","for item in sentence_dataset.take(1):\r\n","    print(idx2word[item.numpy()])\r\n","    print(item.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u2b1u0OFCkPJ"},"source":["seq_length를 25로 ㅅ걸정하여 25개의 단어가 주어졌을 때 다음단어를 예측하도록 데이터를 만들려고 한다.\r\n","\r\n","    sentence_dataset = tf.dat.Dataset.from_tensor_slices(text_as_int)\r\n"," \r\n","Dataset를 생성하는 코드는 위와 같이 한 줄로 간단하다.\r\n","\r\n","    sentence_dataset = sentence_dataset.batch(seq_length+1, drop_remainder=True)\r\n","\r\n","Dataset에 쓰이는 batch() 함수는 Dataset에서 한번에 반환하는 데이터의 숫자를 지정한다. 여기서는 seq_length+1을 지정했는데, 처음의 25개의 단어와 그 뒤에 오는 정답이 도리 1단어를 합쳐서 함께 반환하기 위해서이다. \r\n","\r\n","또 drop_remainder=True 옵션으로 남는 부분은 버린다. 출력에서 의도한 대로 26 단어가 반환되는 것을 확인할 수 있다.\r\n","\r\n","이렇게 만들어진 Dataset으로 새로운 Dataset을 만들어 본다. \r\n","\r\n","26개의 단어가 각각 입력과 정받으로 묶여서 ([25단어],1단어) 형태의 데이터를 반환하게 만드는 Dataset을 만든다.\r\n"]},{"cell_type":"code","metadata":{"id":"WGZz8RPYDgj9"},"source":["def split_input_target(chunk):\r\n","    return[chunk[:-1], chunk[-1]]\r\n","\r\n","train_dataset = sentence_dataset.map(split_input_target)\r\n","for x,y in train_dataset.take(1):\r\n","    print(idx2word[x.numpy()])\r\n","    print(x.numpy())\r\n","    print(idx2word[y.numpy()])\r\n","    print(y.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VYs93ANrEGdG"},"source":["먼저 26개의 단어를 25단어, 1단어로 잘라주는 함수를 정의한 다음, 이 함수를 기존 sentence_dataset에 map()함수를 이요해 새로운 Dataset인 train_dataset을 만든다.\r\n","\r\n","그 다음으로 Dataset의 데이터를 섞고 batch_size를 다시 설정한다."]},{"cell_type":"code","metadata":{"id":"s-3H5zYsEFr_"},"source":["BATCH_SIZE = 128\r\n","setps_per_epoch =  examples_per_epoch // BATCH_SIZE\r\n","BUFFER_SIZE = 10000\r\n","\r\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aB5K_FGkE3Dq"},"source":["빠른 학습을 위해 한 번에 128개의 데이터를 학습하게 하고, 데이터를 섞을 때의 BUFFER_SIZE는 10000으로 설정한다.\r\n","\r\n","tf.data는 이론적으로 무한한 데이터에 대해 대응 가능하기 때문에 한번에 모든 데이터를 섞지 않는다. 따라서 버퍼에 일정한 양의 데이터를 올려놓고 섞는데, 그 사이즈를10000으로 설정한 것이다. "]},{"cell_type":"code","metadata":{"id":"mEXv_cUUFHYJ"},"source":["total_words = len(vocab)\r\n","model = tf.keras.Sequential([\r\n","                             tf.keras.layers.Embedding(total_words, 100, input_length=seq_length),\r\n","                             tf.keras.layers.LSTM(units=100, return_sequences=True),\r\n","                             tf.keras.layers.Dropout(0.2),\r\n","                             tf.keras.layers.LSTM(units=100),\r\n","                             tf.keras.layers.Dense(total_words, activation='softmax')\r\n","])\r\n","\r\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jYzTHuXF0Ro"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","\r\n","def testmodel(epoch, logs):\r\n","    if epoch % 5 != 0 and epoch != 49:\r\n","        return\r\n","    test_sentence = train_text[0]\r\n","\r\n","    next_words = 100\r\n","    for _ in range(next_words):\r\n","        test_text_X = test_sentence.split(' ')[-seq_length:]\r\n","        test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\r\n","        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\r\n","\r\n","        output_idx = model.predict_classes(test_text_X)\r\n","        test_sentence += ' ' + idx2word[output_idx[0]]\r\n","\r\n","    print()\r\n","    print(test_sentence)\r\n","    print()\r\n","\r\n","testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\r\n","\r\n","history = model.fit(train_dataset.repeat(), epochs=50, steps_per_epoch = setps_per_epoch, callbacks=[testmodelcb], verbose=2)"],"execution_count":null,"outputs":[]}]}